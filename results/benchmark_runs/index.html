<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HDH-Bench: Healthcare Data Harmonization Benchmark Results</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.7/dist/chart.umd.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #1a1a2e;
            background: #f8f9fc;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 24px;
        }

        header {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            color: #fff;
            padding: 48px 0 40px;
            margin-bottom: 40px;
        }

        header h1 {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 8px;
            letter-spacing: -0.5px;
        }

        .subtitle {
            font-size: 16px;
            opacity: 0.85;
            margin-bottom: 24px;
        }

        .metadata {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 16px;
        }

        .metadata-item {
            background: rgba(255,255,255,0.1);
            border-radius: 8px;
            padding: 12px 16px;
        }

        .metadata-item label {
            display: block;
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            opacity: 0.7;
            margin-bottom: 4px;
        }

        .metadata-item .value {
            font-size: 15px;
            font-weight: 600;
        }

        .section {
            background: #fff;
            border-radius: 12px;
            padding: 32px;
            margin-bottom: 24px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.06);
        }

        h2 {
            font-size: 20px;
            font-weight: 700;
            margin-bottom: 20px;
            color: #1a1a2e;
        }

        h3 {
            font-size: 16px;
            font-weight: 600;
            margin-top: 24px;
            margin-bottom: 12px;
            color: #16213e;
        }

        h4 {
            font-size: 14px;
            font-weight: 600;
            margin-top: 20px;
            margin-bottom: 8px;
            color: #0f3460;
        }

        p {
            margin-bottom: 12px;
            color: #374151;
            line-height: 1.7;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 14px;
        }

        th, td {
            border: 1px solid #e5e7eb;
            padding: 10px 12px;
            text-align: left;
        }

        th {
            background: #f3f4f6;
            font-weight: 600;
            text-align: center;
            color: #1a1a2e;
            font-size: 13px;
        }

        td {
            text-align: center;
        }

        td.text-left {
            text-align: left;
        }

        .chart-row {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 24px;
            margin: 24px 0;
        }

        .chart-box {
            background: #fff;
            border-radius: 12px;
            padding: 24px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.06);
        }

        .chart-box h3 {
            margin-top: 0;
            margin-bottom: 16px;
            font-size: 15px;
        }

        .chart-box canvas {
            max-height: 300px;
        }

        ul {
            margin: 12px 0 12px 24px;
            color: #374151;
        }

        li {
            margin-bottom: 6px;
            line-height: 1.6;
        }

        code {
            font-family: 'SF Mono', 'Fira Code', monospace;
            background: #f3f4f6;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 13px;
        }

        pre {
            background: #1a1a2e;
            color: #e2e8f0;
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 12px 0;
            font-size: 13px;
            font-family: 'SF Mono', 'Fira Code', monospace;
        }

        .score-high { color: #059669; font-weight: 600; }
        .score-mid { color: #d97706; font-weight: 600; }
        .score-low { color: #dc2626; font-weight: 600; }

        .run-links {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
            margin: 16px 0;
        }

        .run-link {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: #f3f4f6;
            border-radius: 8px;
            text-decoration: none;
            color: #1a1a2e;
            font-size: 13px;
            font-weight: 500;
            transition: background 0.2s;
        }

        .run-link:hover {
            background: #e5e7eb;
        }

        .status-complete { color: #059669; }
        .status-incomplete { color: #d97706; }

        footer {
            text-align: center;
            padding: 32px 0;
            color: #9ca3af;
            font-size: 13px;
        }

        footer a {
            color: #6b7280;
            text-decoration: none;
        }

        footer a:hover {
            color: #1a1a2e;
        }

        .abstract {
            background: #f0f4ff;
            border-left: 4px solid #0f3460;
            padding: 20px 24px;
            border-radius: 0 8px 8px 0;
            margin: 20px 0;
        }

        .abstract p {
            margin-bottom: 0;
        }

        .kpi-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 16px;
            margin: 20px 0;
        }

        .kpi-card {
            background: #f8f9fc;
            border-radius: 8px;
            padding: 20px;
            text-align: center;
        }

        .kpi-value {
            font-size: 32px;
            font-weight: 700;
            color: #1a1a2e;
        }

        .kpi-label {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: #6b7280;
            margin-top: 4px;
        }

        @media (max-width: 768px) {
            .chart-row {
                grid-template-columns: 1fr;
            }
            header {
                padding: 32px 0;
            }
            header h1 {
                font-size: 22px;
            }
            .section {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>HDH-Bench: Healthcare Data Harmonization Benchmark</h1>
            <div class="subtitle">Evaluating AI Agents on Multi-System FHIR Data Integration Tasks</div>
            <div class="metadata">
                <div class="metadata-item">
                    <label>Benchmark Version</label>
                    <div class="value">1.0.0</div>
                </div>
                <div class="metadata-item">
                    <label>Test Date</label>
                    <div class="value">2026-02-04 &ndash; 02-05</div>
                </div>
                <div class="metadata-item">
                    <label>Systems Tested</label>
                    <div class="value">6 FHIR R4 APIs</div>
                </div>
                <div class="metadata-item">
                    <label>Total Runs</label>
                    <div class="value">6 (4 agents)</div>
                </div>
                <div class="metadata-item">
                    <label>Tasks</label>
                    <div class="value">6 benchmark tasks</div>
                </div>
            </div>
        </div>
    </header>

    <div class="container">

        <div class="abstract">
            <p><strong>Abstract:</strong> This report presents the results of HDH-Bench, a benchmark designed to evaluate AI agents on healthcare data integration tasks. The benchmark tests agent capabilities across six distinct clinical scenarios requiring discovery, querying, and harmonization of data from multiple FHIR R4-compliant healthcare systems (EHR, LIS, RIS, Pharmacy, PAS, Billing). Results are validated against ground truth data derived from a master patient index.</p>
        </div>

        <!-- KPI Summary -->
        <div class="kpi-grid">
            <div class="kpi-card">
                <div class="kpi-value">42.3%</div>
                <div class="kpi-label">Opus 4.6 (Claude Code)</div>
            </div>
            <div class="kpi-card">
                <div class="kpi-value">41.3%</div>
                <div class="kpi-label">Mean Accuracy (Sonnet 4.5)</div>
            </div>
            <div class="kpi-card">
                <div class="kpi-value">100%</div>
                <div class="kpi-label">Best Task (Q002)</div>
            </div>
            <div class="kpi-card">
                <div class="kpi-value">50</div>
                <div class="kpi-label">Tasks Completed (Opus)</div>
            </div>
        </div>

        <!-- Charts -->
        <div class="chart-row">
            <div class="chart-box">
                <h3>Task Performance Across Runs</h3>
                <canvas id="taskChart"></canvas>
            </div>
            <div class="chart-box">
                <h3>Multi-Agent Comparison</h3>
                <canvas id="agentChart"></canvas>
            </div>
        </div>

        <div class="chart-row">
            <div class="chart-box">
                <h3>Score Distribution by Run</h3>
                <canvas id="runChart"></canvas>
            </div>
            <div class="chart-box">
                <h3>API Calls vs Performance</h3>
                <canvas id="scatterChart"></canvas>
            </div>
        </div>

        <!-- Executive Summary -->
        <div class="section">
            <h2>1. Executive Summary</h2>

            <p>Four independent test runs were conducted using Claude Sonnet 4.5 (Anthropic) with a fifth run using Claude Code with Opus 4.6. The Sonnet 4.5 runs demonstrate substantial variance with an average accuracy of 41.3% (&sigma; = 6.8%). Claude Code (Opus 4.6) achieved 42.3% accuracy and completed all 50 HDH-Bench tasks (vs. 6 core tasks for other agents).</p>

            <table>
                <thead>
                    <tr>
                        <th>Agent</th>
                        <th>Run 1</th>
                        <th>Run 2</th>
                        <th>Run 3</th>
                        <th>Run 4</th>
                        <th>Mean</th>
                        <th>Std Dev</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="text-left">Claude Sonnet 4.5</td>
                        <td>38.3%</td>
                        <td>47.5%</td>
                        <td>47.1%</td>
                        <td>32.1%</td>
                        <td class="score-mid">41.3%</td>
                        <td>6.8%</td>
                    </tr>
                    <tr>
                        <td class="text-left">Claude Code (Opus 4.6)</td>
                        <td>42.3%</td>
                        <td>&mdash;</td>
                        <td>&mdash;</td>
                        <td>&mdash;</td>
                        <td class="score-mid">42.3%</td>
                        <td>&mdash;</td>
                    </tr>
                </tbody>
            </table>

            <h3>1.1 Multi-Agent Comparison</h3>

            <p>In addition to the primary Claude Sonnet 4.5 evaluation, comparative testing was conducted with Claude Code (Opus 4.6) and two alternative agent implementations using OpenAI models.</p>

            <table>
                <thead>
                    <tr>
                        <th>Agent Implementation</th>
                        <th>Test Runs</th>
                        <th>Overall Mean Accuracy</th>
                        <th>Status</th>
                        <th>Notes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="text-left">Claude Code (Opus 4.6)</td>
                        <td>1</td>
                        <td class="score-mid">42.3%</td>
                        <td class="status-complete">Complete</td>
                        <td class="text-left">50/50 tasks, Q001: 100%, Q002: 100%</td>
                    </tr>
                    <tr>
                        <td class="text-left">Claude Sonnet 4.5 (Anthropic)</td>
                        <td>3</td>
                        <td class="score-mid">44.3%</td>
                        <td class="status-complete">Complete</td>
                        <td class="text-left">Q001: 94.4%, Q002: 100%</td>
                    </tr>
                    <tr>
                        <td class="text-left">GPT-5.2 (OpenAI Agents SDK)</td>
                        <td>1</td>
                        <td class="score-low">21.3%</td>
                        <td class="status-incomplete">Incomplete</td>
                        <td class="text-left">API quota exceeded</td>
                    </tr>
                    <tr>
                        <td class="text-left">GPT-4o (OpenAI Direct API)</td>
                        <td>1</td>
                        <td class="score-low">8.2%</td>
                        <td class="status-incomplete">Incomplete</td>
                        <td class="text-left">API quota exceeded</td>
                    </tr>
                </tbody>
            </table>

            <p>Claude Code (Opus 4.6) achieved 42.3% accuracy and is the only agent to complete all 50 HDH-Bench tasks. Claude Sonnet 4.5 averaged 44.3% across 3 runs on the 6 core tasks. Both Claude agents outperform GPT-5.2 (2x) and GPT-4o (5x), though OpenAI evaluations were prematurely terminated due to API quota constraints.</p>
        </div>

        <!-- Methodology -->
        <div class="section">
            <h2>2. Methodology</h2>

            <h3>2.1 Benchmark Design</h3>
            <p>HDH-Bench consists of six tasks designed to evaluate different aspects of healthcare data integration:</p>
            <ul>
                <li><strong>Q001 - Patient 360 View:</strong> Cross-system patient record matching across 6 healthcare systems</li>
                <li><strong>Q002 - Diabetic Cohort Building:</strong> Multi-system cohort identification using diagnosis and medication data</li>
                <li><strong>Q003 - Abnormal Laboratory Results:</strong> Clinical data filtering for patients with HbA1c &gt; 9.0%</li>
                <li><strong>Q004 - Duplicate Patient Detection:</strong> Identification of duplicate patient records using demographic matching</li>
                <li><strong>Q005 - Cross-System Cohort Validation:</strong> Complex multi-constraint query with cross-system validation</li>
                <li><strong>Q006 - Data Quality Assessment:</strong> Detection of orphaned results and abandoned orders</li>
            </ul>

            <h3>2.2 Systems Under Test</h3>
            <p>The benchmark environment consists of six independent FHIR R4-compliant API endpoints representing typical healthcare system architecture: Electronic Health Record (EHR), Laboratory Information System (LIS), Radiology Information System (RIS), Pharmacy Management, Patient Administration System (PAS), and Billing System.</p>

            <h3>2.3 Validation Methodology</h3>
            <p>All agent responses are validated against ground truth data maintained in a master patient index. Scoring is performed automatically using task-specific validation criteria. Pass/fail thresholds vary by task complexity and clinical requirements.</p>
        </div>

        <!-- Results -->
        <div class="section">
            <h2>3. Results</h2>

            <h3>3.1 Overall Performance</h3>

            <table>
                <thead>
                    <tr>
                        <th>Run ID</th>
                        <th>Overall Score</th>
                        <th>Execution Time</th>
                        <th>API Calls</th>
                        <th>Tasks Passed</th>
                        <th>Configuration</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><a href="20260204_172804/index.html">Run 1</a></td>
                        <td class="score-mid">38.3%</td>
                        <td>200s</td>
                        <td>41</td>
                        <td>2/6</td>
                        <td class="text-left">Claude Sonnet 4.5 &mdash; Python 3.10 + SDK 0.28.0</td>
                    </tr>
                    <tr>
                        <td><a href="20260204_173323/index.html">Run 2</a></td>
                        <td class="score-mid">47.5%</td>
                        <td>302s</td>
                        <td>68</td>
                        <td>2/6</td>
                        <td class="text-left">Claude Sonnet 4.5 &mdash; Python 3.10 + SDK 0.28.0</td>
                    </tr>
                    <tr>
                        <td><a href="20260204_183944/index.html">Run 3</a></td>
                        <td class="score-mid">47.1%</td>
                        <td>329s</td>
                        <td>75</td>
                        <td>2/6</td>
                        <td class="text-left">Claude Sonnet 4.5 &mdash; Python 3.10 + SDK 0.28.0</td>
                    </tr>
                    <tr>
                        <td><a href="20260204_191140/index.html">Run 4</a></td>
                        <td class="score-low">32.1%</td>
                        <td>312s</td>
                        <td>71</td>
                        <td>2/6</td>
                        <td class="text-left">Claude Sonnet 4.5 &mdash; Python 3.11.11 + SDK 0.77.1</td>
                    </tr>
                    <tr>
                        <td><a href="20260204_193157/index.html">Run 5</a></td>
                        <td class="score-mid">38.0%</td>
                        <td>--</td>
                        <td>--</td>
                        <td>2/6</td>
                        <td class="text-left">Claude-Direct Agent (Sonnet 4.5)</td>
                    </tr>
                    <tr>
                        <td><a href="20260205_claude_code_opus/index.html">Run 6</a></td>
                        <td class="score-mid">42.3%</td>
                        <td>--</td>
                        <td>--</td>
                        <td>2/6</td>
                        <td class="text-left">Claude Code (Opus 4.6)</td>
                    </tr>
                </tbody>
            </table>

            <h3>3.2 Task-Level Performance</h3>

            <table>
                <thead>
                    <tr>
                        <th rowspan="2">Task</th>
                        <th colspan="4">Claude Sonnet 4.5 (%)</th>
                        <th rowspan="2">Opus 4.6</th>
                        <th rowspan="2">Mean</th>
                        <th rowspan="2">Pass Rate</th>
                    </tr>
                    <tr>
                        <th>Run 1</th>
                        <th>Run 2</th>
                        <th>Run 3</th>
                        <th>Run 4</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="text-left">Q001: Patient 360 View</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">83.3</td>
                        <td class="score-low">0.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-mid">76.7</td>
                        <td>4/5</td>
                    </tr>
                    <tr>
                        <td class="text-left">Q002: Diabetic Cohort</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td class="score-high">100.0</td>
                        <td>5/5</td>
                    </tr>
                    <tr>
                        <td class="text-left">Q003: Abnormal Glucose</td>
                        <td class="score-low">30.0</td>
                        <td class="score-mid">40.7</td>
                        <td class="score-mid">56.3</td>
                        <td class="score-high">90.1</td>
                        <td class="score-low">0.0</td>
                        <td class="score-mid">43.4</td>
                        <td>1/5</td>
                    </tr>
                    <tr>
                        <td class="text-left">Q004: Duplicate Detection</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">16.7</td>
                        <td class="score-low">3.3</td>
                        <td>0/5</td>
                    </tr>
                    <tr>
                        <td class="text-left">Q005: Cross-System Cohort</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td class="score-low">0.0</td>
                        <td>0/5</td>
                    </tr>
                    <tr>
                        <td class="text-left">Q006: Data Quality Issues</td>
                        <td class="score-low">0.0</td>
                        <td class="score-mid">44.0</td>
                        <td class="score-mid">42.8</td>
                        <td class="score-low">2.5</td>
                        <td class="score-mid">37.3</td>
                        <td class="score-low">25.3</td>
                        <td>0/5</td>
                    </tr>
                </tbody>
            </table>

            <h3>3.3 Detailed Task Analysis</h3>

            <h4>Q001: Patient 360 View (Cross-System Matching)</h4>
            <p><strong>Objective:</strong> Identify all records for patient MRN-100042 across six healthcare systems.</p>
            <p><strong>Performance:</strong> Mean accuracy 76.7% across 5 runs. Claude Code (Opus 4.6) achieved 100%, matching all 6 system IDs correctly. Sonnet 4.5 succeeded in Runs 1-3 (100%, 100%, 83.3%) but failed in Run 4 (0%).</p>

            <h4>Q002: Diabetic Cohort Building</h4>
            <p><strong>Objective:</strong> Identify all patients with Type 2 diabetes on metformin therapy.</p>
            <p><strong>Performance:</strong> Mean accuracy 100% across all 5 runs. Claude Code (Opus 4.6) also achieved 100%, finding 134 patients (ground truth range: 92-154). This task is deterministically solved by all Claude agents.</p>

            <h4>Q003: Abnormal Laboratory Results</h4>
            <p><strong>Objective:</strong> Identify patients with HbA1c test results greater than 9.0%.</p>
            <p><strong>Performance:</strong> Mean accuracy 43.4% across 5 runs. Claude Code (Opus 4.6) scored 0% by finding 143 patients with any abnormal glucose (broader interpretation) vs. the expected 71 HbA1c-specific patients. Sonnet 4.5 Run 4 achieved the best score at 90.1%.</p>

            <h4>Q004: Duplicate Patient Detection</h4>
            <p><strong>Objective:</strong> Identify duplicate patient records using demographic matching algorithms.</p>
            <p><strong>Performance:</strong> Mean accuracy 3.3% across 5 runs. Claude Code (Opus 4.6) made the first non-zero progress on this task at 16.7%, finding 18 high-confidence duplicate groups (vs. expected 39-59). All Sonnet 4.5 runs scored 0%.</p>

            <h4>Q005: Cross-System Cohort with Validation</h4>
            <p><strong>Objective:</strong> Identify diabetic patients with recent HbA1c tests, validated across all systems.</p>
            <p><strong>Performance:</strong> Mean accuracy 0% across all 5 runs. Claude Code (Opus 4.6) identified 4 matching patients via a multi-criteria pipeline but did not provide cross-system IDs required for validation scoring.</p>

            <h4>Q006: Data Quality Assessment</h4>
            <p><strong>Objective:</strong> Identify orphaned laboratory results and abandoned medication orders.</p>
            <p><strong>Performance:</strong> Mean accuracy 25.3% across 5 runs. Claude Code (Opus 4.6) scored 37.3% (orphaned: 148 found, 74.6% score; abandoned: 40 found, 0% score). The agent used broader criteria for both categories, consistent with its expansive interpretation pattern.</p>
        </div>

        <!-- Discussion -->
        <div class="section">
            <h2>4. Discussion</h2>

            <h3>4.1 Key Findings</h3>
            <ul>
                <li><strong>Claude Code (Opus 4.6) Competitive:</strong> At 42.3%, Opus 4.6 via Claude Code is competitive with Sonnet 4.5's mean of 41.3%, while being the only agent to complete all 50 HDH-Bench tasks.</li>
                <li><strong>First Q004 Progress:</strong> Claude Code (Opus 4.6) achieved 16.7% on duplicate detection (Q004), the first non-zero score on this task. It found 18 high-confidence duplicate groups vs. the expected 39-59.</li>
                <li><strong>Broader Interpretation Pattern:</strong> Opus 4.6 consistently interprets task criteria more broadly than expected (Q003: all abnormal glucose vs. HbA1c-specific; Q006: broader orphan definition), suggesting a tendency toward recall over precision.</li>
                <li><strong>High Performance Variance:</strong> Overall accuracy ranges from 32.1% to 47.5% across Sonnet 4.5 runs (standard deviation 6.8%), demonstrating substantial non-determinism.</li>
                <li><strong>Q002 Perfect Consistency:</strong> Diabetic cohort identification maintained 100% accuracy across all 5 runs (including Opus 4.6), confirming this task class is deterministically solved.</li>
                <li><strong>Complex Task Limitations:</strong> Q005 remains at 0% across all agents. Q004 saw first progress with Opus 4.6 but remains unsolved.</li>
            </ul>

            <h3>4.2 Limitations</h3>
            <ul>
                <li>Limited to FHIR R4 API interactions; other healthcare data formats not evaluated</li>
                <li>Synthetic test data; real-world data variability not captured</li>
                <li>OpenAI agent comparisons were limited by API quota constraints and should be considered preliminary</li>
            </ul>
        </div>

        <!-- Conclusion -->
        <div class="section">
            <h2>5. Conclusion</h2>
            <p>Both Claude Sonnet 4.5 and Claude Code (Opus 4.6) demonstrate capability for basic healthcare data integration tasks, achieving 100% accuracy on cohort identification and strong patient matching. Opus 4.6 is the first agent to make progress on duplicate detection (Q004) and completed all 50 HDH-Bench tasks. Performance on complex tasks requiring cross-system validation (Q005) remains unsolved across all agents, suggesting fundamental architectural limitations in multi-system reasoning.</p>
        </div>

        <!-- Technical Specs -->
        <div class="section">
            <h2>6. Technical Specifications</h2>
            <table>
                <thead>
                    <tr><th>Parameter</th><th>Value</th></tr>
                </thead>
                <tbody>
                    <tr><td class="text-left">Agent Models</td><td class="text-left">Claude Sonnet 4.5, Claude Opus 4.6, GPT-5.2, GPT-4o</td></tr>
                    <tr><td class="text-left">API Protocol</td><td class="text-left">Anthropic Direct API, Claude Code CLI, OpenAI Agents SDK</td></tr>
                    <tr><td class="text-left">FHIR Version</td><td class="text-left">R4 (4.0.1)</td></tr>
                    <tr><td class="text-left">Validation Method</td><td class="text-left">Ground truth comparison against master patient index</td></tr>
                    <tr><td class="text-left">Test Environment</td><td class="text-left">Local FHIR servers (6 independent systems)</td></tr>
                    <tr><td class="text-left">Total Test Runs</td><td class="text-left">6 (across 4 agent implementations)</td></tr>
                </tbody>
            </table>
        </div>

        <!-- Reproducibility -->
        <div class="section">
            <h2>7. Reproducibility</h2>

            <p>All benchmark runs are stored in versioned directories with complete validation data. Each run contains metadata, raw agent responses, ground truth validation, and a human-readable summary.</p>

            <h3>Run Data</h3>
            <div class="run-links">
                <a class="run-link" href="20260204_172804/index.html">Run 1: 38.3%</a>
                <a class="run-link" href="20260204_173323/index.html">Run 2: 47.5%</a>
                <a class="run-link" href="20260204_183944/index.html">Run 3: 47.1%</a>
                <a class="run-link" href="20260204_191140/index.html">Run 4: 32.1%</a>
                <a class="run-link" href="20260204_193157/index.html">Run 5: 38.0%</a>
                <a class="run-link" href="20260205_claude_code_opus/index.html">Run 6 (Opus 4.6): 42.3%</a>
            </div>

            <p>Benchmark execution command:</p>
            <pre>python3 scripts/run_validated_benchmark.py --agents claude --all-tasks</pre>
        </div>

    </div>

    <footer>
        <p>Generated: 2026-02-05 | <a href="https://github.com/jonathanwmaddison/healthcare-data-poc">healthcare-data-poc</a> | HDH-Bench v1.0.0</p>
    </footer>

    <script>
        const colors = {
            blue: 'rgba(15, 52, 96, 0.8)',
            blueLight: 'rgba(15, 52, 96, 0.15)',
            green: 'rgba(5, 150, 105, 0.8)',
            orange: 'rgba(217, 119, 6, 0.8)',
            red: 'rgba(220, 38, 38, 0.8)',
            purple: 'rgba(124, 58, 237, 0.8)',
            gray: 'rgba(107, 114, 128, 0.8)',
        };

        // Task Performance Radar Chart
        new Chart(document.getElementById('taskChart'), {
            type: 'radar',
            data: {
                labels: ['Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006'],
                datasets: [
                    {
                        label: 'Run 1',
                        data: [100, 100, 30, 0, 0, 0],
                        borderColor: colors.blue,
                        backgroundColor: 'rgba(15, 52, 96, 0.05)',
                        borderWidth: 2,
                        pointRadius: 3,
                    },
                    {
                        label: 'Run 2',
                        data: [100, 100, 40.7, 0, 0, 44],
                        borderColor: colors.green,
                        backgroundColor: 'rgba(5, 150, 105, 0.05)',
                        borderWidth: 2,
                        pointRadius: 3,
                    },
                    {
                        label: 'Run 3',
                        data: [83.3, 100, 56.3, 0, 0, 42.8],
                        borderColor: colors.orange,
                        backgroundColor: 'rgba(217, 119, 6, 0.05)',
                        borderWidth: 2,
                        pointRadius: 3,
                    },
                    {
                        label: 'Run 4',
                        data: [0, 100, 90.1, 0, 0, 2.5],
                        borderColor: colors.purple,
                        backgroundColor: 'rgba(124, 58, 237, 0.05)',
                        borderWidth: 2,
                        pointRadius: 3,
                    },
                    {
                        label: 'Opus 4.6',
                        data: [100, 100, 0, 16.7, 0, 37.3],
                        borderColor: 'rgba(220, 38, 38, 0.8)',
                        backgroundColor: 'rgba(220, 38, 38, 0.05)',
                        borderWidth: 2,
                        pointRadius: 3,
                        borderDash: [5, 5],
                    }
                ]
            },
            options: {
                responsive: true,
                scales: {
                    r: {
                        beginAtZero: true,
                        max: 100,
                        ticks: { stepSize: 25, font: { size: 10 } },
                        pointLabels: { font: { size: 12, weight: '600' } }
                    }
                },
                plugins: {
                    legend: { position: 'bottom', labels: { font: { size: 11 } } }
                }
            }
        });

        // Agent Comparison Bar Chart
        new Chart(document.getElementById('agentChart'), {
            type: 'bar',
            data: {
                labels: ['Claude Opus 4.6', 'Claude Sonnet 4.5', 'GPT-5.2', 'GPT-4o'],
                datasets: [{
                    label: 'Mean Accuracy (%)',
                    data: [42.3, 44.3, 21.3, 8.2],
                    backgroundColor: ['rgba(220, 38, 38, 0.8)', colors.blue, colors.orange, colors.gray],
                    borderRadius: 6,
                    barPercentage: 0.6,
                }]
            },
            options: {
                responsive: true,
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 100,
                        ticks: { callback: v => v + '%' }
                    }
                },
                plugins: {
                    legend: { display: false },
                    tooltip: { callbacks: { label: ctx => ctx.parsed.y + '%' } }
                }
            }
        });

        // Run Score Line Chart
        new Chart(document.getElementById('runChart'), {
            type: 'line',
            data: {
                labels: ['Run 1', 'Run 2', 'Run 3', 'Run 4', 'Run 5', 'Run 6 (Opus)'],
                datasets: [{
                    label: 'Claude Sonnet 4.5',
                    data: [38.3, 47.5, 47.1, 32.1, 38.0, null],
                    borderColor: colors.blue,
                    backgroundColor: colors.blueLight,
                    fill: false,
                    tension: 0.3,
                    pointRadius: 5,
                    pointBackgroundColor: colors.blue,
                    borderWidth: 2,
                    spanGaps: false,
                },
                {
                    label: 'Claude Opus 4.6',
                    data: [null, null, null, null, null, 42.3],
                    borderColor: 'rgba(220, 38, 38, 0.8)',
                    backgroundColor: 'rgba(220, 38, 38, 0.15)',
                    fill: false,
                    pointRadius: 8,
                    pointBackgroundColor: 'rgba(220, 38, 38, 0.8)',
                    borderWidth: 2,
                    pointStyle: 'triangle',
                }]
            },
            options: {
                responsive: true,
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 60,
                        ticks: { callback: v => v + '%' }
                    }
                },
                plugins: {
                    legend: { position: 'bottom', labels: { font: { size: 11 } } },
                    tooltip: { callbacks: { label: ctx => ctx.dataset.label + ': ' + ctx.parsed.y + '%' } }
                }
            }
        });

        // API Calls vs Performance Scatter
        new Chart(document.getElementById('scatterChart'), {
            type: 'scatter',
            data: {
                datasets: [{
                    label: 'Claude Runs',
                    data: [
                        { x: 41, y: 38.3 },
                        { x: 68, y: 47.5 },
                        { x: 75, y: 47.1 },
                        { x: 71, y: 32.1 },
                    ],
                    backgroundColor: colors.blue,
                    pointRadius: 8,
                    pointHoverRadius: 10,
                }]
            },
            options: {
                responsive: true,
                scales: {
                    x: {
                        title: { display: true, text: 'API Calls', font: { size: 12 } },
                        min: 30,
                        max: 85,
                    },
                    y: {
                        title: { display: true, text: 'Overall Score (%)', font: { size: 12 } },
                        beginAtZero: true,
                        max: 60,
                        ticks: { callback: v => v + '%' }
                    }
                },
                plugins: {
                    legend: { display: false },
                    tooltip: {
                        callbacks: {
                            label: ctx => `${ctx.parsed.x} calls, ${ctx.parsed.y}%`
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
